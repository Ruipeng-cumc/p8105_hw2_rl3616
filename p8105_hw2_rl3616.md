p8105_hw2_rl3616
================
Ruipeng Li
2025-10-01

Loading packages.

``` r
library(tidyverse)
library(lubridate)      #to use parse_date_time
library(readxl)         #for problem 2
library(dplyr)          #filter
```

# Problem 1

#### Part 1

Clean the data in pols-month.csv.

``` r
pols_df <- read_csv("data/pols-month.csv") |>
  janitor::clean_names() |>
  separate(mon, into = c("year", "month", "day"), convert = TRUE) |>
  mutate(
    month = month.name[month],
    president = ifelse(prez_gop == 1, "gop", "dem")
    ) |>
  select(-prez_gop, -prez_dem, -day) |> 
  arrange(year, factor(month, levels = month.name))   #Old data to new data
```

Finished part one, let’s looking at our new pols-month.

``` r
pols_df
```

    ## # A tibble: 822 × 9
    ##     year month     gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president
    ##    <int> <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>    
    ##  1  1947 January        23      51     253      23      45     198 dem      
    ##  2  1947 February       23      51     253      23      45     198 dem      
    ##  3  1947 March          23      51     253      23      45     198 dem      
    ##  4  1947 April          23      51     253      23      45     198 dem      
    ##  5  1947 May            23      51     253      23      45     198 dem      
    ##  6  1947 June           23      51     253      23      45     198 dem      
    ##  7  1947 July           23      51     253      23      45     198 dem      
    ##  8  1947 August         23      51     253      23      45     198 dem      
    ##  9  1947 September      23      51     253      23      45     198 dem      
    ## 10  1947 October        23      51     253      23      45     198 dem      
    ## # ℹ 812 more rows

#### Part 2

clean the data in snp.csv This is pretty complicate since we both have
yyyy/m/d type of date and m/d/yy

``` r
snp_df <- read_csv("data/snp.csv") |>
  janitor::clean_names() |>
  mutate(date = parse_date_time(date, orders = c("ymd", "mdy"))) |>     #in this way I don't need to relocate()
  separate(date, into = c("year", "month", "day"), convert = TRUE) |>
  mutate(
    year  = if_else(year > 2015, year - 100L, year),
    month = month.name[month]
    ) |> 
  select(-day) |> 
  arrange(year, factor(month, levels = month.name))   #Old data to new data
```

I found “parse_date_time()” in lubridate package can both help me clean
the “ymd” and “mdy” into “y-m-d” type, and arrange and organize the
data, I don’t know is that always happens? but I’m feeling complicate in
using “skip” and something more like part one.

``` r
snp_df
```

    ## # A tibble: 787 × 3
    ##     year month     close
    ##    <int> <chr>     <dbl>
    ##  1  1950 January    17.0
    ##  2  1950 February   17.2
    ##  3  1950 March      17.3
    ##  4  1950 April      18.0
    ##  5  1950 May        18.8
    ##  6  1950 June       17.7
    ##  7  1950 July       17.8
    ##  8  1950 August     18.4
    ##  9  1950 September  19.5
    ## 10  1950 October    19.5
    ## # ℹ 777 more rows

#### Part 3

clean the data in unemployment.csv

``` r
unemp_df <- read_csv("data/unemployment.csv") |>
  janitor::clean_names() |>
  pivot_longer(
    cols = jan:dec,                 
    names_to = "month",
    values_to = "unemployment"
  ) |>
  mutate(
    month = month.name[match(month, tolower(month.abb))]       #Use tolower() to match the month in lowercase.
  ) |>
  arrange(year, factor(month, levels = month.name))   #Old data to new data
```

``` r
unemp_df
```

    ## # A tibble: 816 × 3
    ##     year month     unemployment
    ##    <dbl> <chr>            <dbl>
    ##  1  1948 January            3.4
    ##  2  1948 February           3.8
    ##  3  1948 March              4  
    ##  4  1948 April              3.9
    ##  5  1948 May                3.5
    ##  6  1948 June               3.6
    ##  7  1948 July               3.6
    ##  8  1948 August             3.9
    ##  9  1948 September          3.8
    ## 10  1948 October            3.7
    ## # ℹ 806 more rows

#### Combaining all together

``` r
final_df <- pols_df |>
  left_join(snp_df, by = c("year","month")) |>
  left_join(unemp_df, by = c("year","month"))
```

#### Conclusion

The `pols-month` table records the number of governors, senators, and
representatives from each party during the presidential elections of
different parties (Democratic/Republican).

The `snp` table represents stock market trends, and the `close` value
reflects the overall stock price level of large companies.

The `unemployment` table represents the US unemployment rate.

We merged and then split the year and month data into the `final_df`
table. This allows us to compare presidential party, stock market
performance, and unemployment rates for a specific year and month,
making it ideal for studying the relationship between politics, the
economy, and employment.

The dataset spans 1947–2015 and covers many important historical events,
such as the 1973–1975 oil crisis, the 1981–1982 recession, the 2000
dot-com bubble, and the 2007–2009 Great Recession. Therefore, this
dataset provides significant reference value for studying the
relationships between politics, the economy, and employment.

# Problem 2

``` r
path_trash <- "data/202509 Trash Wheel Collection Data.xlsx"

mr <- read_excel(path_trash, sheet = "Mr. Trash Wheel", skip=1, range = cell_cols("A:N"))
prof <- read_excel(path_trash, sheet = "Professor Trash Wheel", skip=1, range = cell_cols("A:M"))
gwyn <- read_excel(path_trash, sheet = "Gwynns Falls Trash Wheel", skip=1, range = cell_cols("A:L"))

mr <- mr |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    wheel = "Mr. Trash Wheel",
    date = ymd(date), 
    year  = as.integer(year(date)),
    month = month.name[month(date)],                                           #in order to unify data
    sports_balls = as.integer(round(sports_balls, 0))
    ) 

prof <- prof |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    wheel = "Professor Trash Wheel",
    date = ymd(date),
    year  = as.integer(year(date)),
    month = month.name[month(date)],                                           #in order to unify data
    ) 

gwyn <- gwyn |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    wheel = "Gwynns Falls Trash Wheel",
    date = ymd(date),
    year  = as.integer(year(date)),
    month = month.name[month(date)],                                           #in order to unify data
    ) 

final_wheels <- bind_rows(mr, prof, gwyn) |>
  arrange(date) |> 
  relocate("wheel","dumpster")
```

I successfully cleaned and combined the datasets for Mr. Trash Wheel,
Professor Trash Wheel, and Gwynnda, resulting in a dataset containing
1188 observations, with each row corresponding to a dumpster collection
event.

(I didn’t delete the `day` column as in Problem 1, as the event records
are daily. I retained the full `date` column for better visualization. I
also debated whether to delete the original `year` and `month`, but for
statistical convenience, I retained them.)

Key variables include the collection `date`, the `weight_tons` of trash
collected, the number of `cigarette_butts`, `plastic_bottles`, and
`sports_balls` (rounded to integers), as well as the estimated number of
`homes_powered`.

``` r
prof_total_weight <- final_wheels |>
  filter(wheel == "Professor Trash Wheel") |>
  summarise(total_weight = sum(weight_tons, na.rm = TRUE)) |>
  pull(total_weight)

gwyn_jun2022_cigs <- final_wheels |>
  filter(wheel == "Gwynns Falls Trash Wheel", year == 2022, month == "June") |> 
  summarise(total_cigrette_butts = sum(cigarette_butts, na.rm = TRUE)) |> 
  pull(total_cigrette_butts)
```

Across all available data, Professor Trash Wheel collected a total of
282.3 tons of trash. In June 2022, Gwynnda collected 18,120 cigarette
butts.

# Problem 3

``` r
zip_zori_df <- read_csv("data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |> 
  janitor::clean_names() |>
  rename(zip_code = region_name) |> 
  select(-region_type, -state_name, -state, -city, -metro) |> 
  pivot_longer(
    cols = starts_with("x20"),
    names_to = "date",
    values_to = "rent_price"
  ) |> 
  filter(!is.na(rent_price)) |>
  mutate(
    county_name = str_replace(county_name, " County", ""),
    date = str_replace(date, "x", ""),
    date = ymd(date)
    )
```

I deleted all the `region_type`, `state_name`, `state`, `city`, and
`metro` column since they are with same variable useless in comparing
two datasets.

``` r
zip_codes_df <- read_csv("data/Zip Codes.csv") |> 
  janitor::clean_names() |>
  rename(county_name = county) |> 
  select(-state_fips, -county_code, -file_date)
```

Similar before, I deleted all the `state_fips`, `county_code` column
since they are shows in `county_fips`, and deleted `file_date` column
since they are with same variable useless in comparing two datasets.

``` r
new_york_zori <- zip_zori_df |>
  left_join(zip_codes_df, by = c("county_name","zip_code")) |> 
  relocate("region_id", "county_name", "county_fips", "zip_code")
```

We successfully combine the two tables and put the important information
first, now step into Completeness & Correctness.

``` r
# 1) check if any zip_code, date has duplicated
dup <- new_york_zori |> 
  count(zip_code, date) |> 
  filter(n > 1)
stopifnot(nrow(dup) == 0)

# 2) check the date and rent to be appropriate  and non-negative
summary_dates <- new_york_zori |>  
  summarise(
    any_neg = any(rent_price < 0, na.rm = TRUE),
    min_date = min(date, na.rm = TRUE),
    max_date = max(date, na.rm = TRUE),
    months   = n_distinct(date)
  )

min_date <- summary_dates$min_date
max_date <- summary_dates$max_date
months <- summary_dates$months
```

As we can see, there’s no duplicated, inappropriate date and price, date
started at 2015-01-31 and end at 2024-08-31. Using calculation
`2024-2015` we got 9 month, add `8` month from `2024`, we have 116
month, which was same as 116 month we got by `n_distinct()`.

In this dataset, we have 10450 rows, 149 different zip code, and 42
different neighborhood.

``` r
#Checking Zip codes
zips_in_zori <- zip_zori_df |> 
  distinct(zip_code)

zips_disappear <- zip_codes_df |> 
  anti_join(zips_in_zori, by = "zip_code")

zips_disappear
```

    ## # A tibble: 171 × 4
    ##    county_name county_fips zip_code neighborhood              
    ##    <chr>             <dbl>    <dbl> <chr>                     
    ##  1 Bronx             36005    10464 Southeast Bronx           
    ##  2 Bronx             36005    10474 Hunts Point and Mott Haven
    ##  3 Bronx             36005    10475 Northeast Bronx           
    ##  4 Bronx             36005    10499 <NA>                      
    ##  5 Bronx             36005    10550 <NA>                      
    ##  6 Bronx             36005    10704 <NA>                      
    ##  7 Bronx             36005    10705 <NA>                      
    ##  8 Bronx             36005    10803 <NA>                      
    ##  9 Kings             36047    11202 <NA>                      
    ## 10 Kings             36047    11224 Southern Brooklyn         
    ## # ℹ 161 more rows

From the chart, we can see that most of these addresses do not point to
neighborhoods (which neighborhoods is NA). Take the ZIP code 10499 as an
example, which appears in the ZIP metadata but not in the Zillow
dataset. This ZIP is a USPS-assigned unique code for postal facilities
in the Bronx, rather than a residential neighborhood. Because such ZIP
codes correspond to government or commercial facilities rather than
housing markets, they are excluded from Zillow’s rental index. Some ZIP
codes with community names (such as 10464) do not appear in Zillow’s
listings, perhaps because they have a small population or users do not
accept rentals.

``` r
# filter data between January 2020 to 2021

jan2020_df <- new_york_zori |> 
  filter(year(date) == 2020, month(date) == 1) |> 
  select(zip_code, county_name, neighborhood, rent_2020 = rent_price)
  
jan2021_df <- new_york_zori |> 
  filter(year(date) == 2021, month(date) == 1) |> 
  select(zip_code, county_name, neighborhood, rent_2021 = rent_price)

# Merge and compare data

covid_comp_df <- jan2020_df |> 
  left_join(jan2021_df, by = c("zip_code", "county_name", "neighborhood")) |> 
  mutate(drop = rent_2021 - rent_2020) |> 
  arrange(drop) |> 
  slice(1:10)
```

Here’s the table that shows the 10 ZIP codes (along with the borough and
neighborhood) with largest drop in price from January 2020 to 2021.

``` r
knitr::kable(covid_comp_df)
```

| zip_code | county_name | neighborhood | rent_2020 | rent_2021 | drop |
|---:|:---|:---|---:|---:|---:|
| 10007 | New York | Lower Manhattan | 6334.211 | 5421.614 | -912.5966 |
| 10069 | New York | NA | 4623.042 | 3874.918 | -748.1245 |
| 10009 | New York | Lower East Side | 3406.442 | 2692.187 | -714.2550 |
| 10016 | New York | Gramercy Park and Murray Hill | 3731.135 | 3019.431 | -711.7045 |
| 10001 | New York | Chelsea and Clinton | 4108.098 | 3397.648 | -710.4499 |
| 10002 | New York | Lower East Side | 3645.416 | 2935.113 | -710.3028 |
| 10004 | New York | Lower Manhattan | 3149.658 | 2443.697 | -705.9608 |
| 10038 | New York | Lower Manhattan | 3573.201 | 2875.616 | -697.5853 |
| 10012 | New York | Greenwich Village and Soho | 3628.566 | 2942.344 | -686.2218 |
| 10010 | New York | Gramercy Park and Murray Hill | 3697.284 | 3012.353 | -684.9304 |

As we can see, rents in Lower Manhattan (ZIP 10007) showed the largest
decline, dropping by about 913 dollars per month from January 2020 to
January 2021. Other substantial declines also occurred in densely
populated Manhattan neighborhoods, where high turnover and a reliance on
rental apartments amplified the effects of the pandemic. This pattern is
consistent with remote work policies and outmigration during COVID-19,
which sharply reduced demand for central-city housing.

#### Little question

Should I delete all the temporary data(like jan2020_df, zips_in_zori)
and only leave the main data(like dfs. final_wheels, new_york_zori) in
the environment? Thank you so much!
